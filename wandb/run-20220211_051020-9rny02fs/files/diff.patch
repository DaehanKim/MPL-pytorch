diff --git a/data.py b/data.py
index 5eec792..88e52a1 100644
--- a/data.py
+++ b/data.py
@@ -5,8 +5,10 @@ import numpy as np
 from PIL import Image
 from torchvision import datasets
 from torchvision import transforms
+from torchvision.transforms import InterpolationMode
 
 from augmentation import RandAugmentCIFAR
+from augment import TrivialAugmentWide
 
 logger = logging.getLogger(__name__)
 
@@ -25,8 +27,11 @@ def get_cifar10(args):
                               padding=int(args.resize * 0.125),
                               fill=128,
                               padding_mode='constant'),
+        # RandAugmentCIFAR(n=2, m=16),
+        TrivialAugmentWide(interpolation=InterpolationMode.BICUBIC, fill=128),
         transforms.ToTensor(),
-        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)
+        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),
+        transforms.RandomErasing(p=1.0, scale=(0.,6/32), ratio=(1.0, 1.0))
     ])
     transform_val = transforms.Compose([
         transforms.ToTensor(),
diff --git a/main.py b/main.py
index d3708ac..082805c 100644
--- a/main.py
+++ b/main.py
@@ -56,7 +56,7 @@ parser.add_argument('--resume', default='', type=str, help='path to checkpoint')
 parser.add_argument('--evaluate', action='store_true', help='only evaluate model on validation set')
 parser.add_argument('--finetune', action='store_true',
                     help='only finetune model on labeled dataset')
-parser.add_argument('--finetune-epochs', default=125, type=int, help='finetune epochs')
+parser.add_argument('--finetune-epochs', default=625, type=int, help='finetune epochs')
 parser.add_argument('--finetune-batch-size', default=512, type=int, help='finetune batch size')
 parser.add_argument('--finetune-lr', default=1e-5, type=float, help='finetune learning late')
 parser.add_argument('--finetune-weight-decay', default=0, type=float, help='finetune weight decay')
@@ -212,15 +212,17 @@ def train_loop(args, labeled_loader, unlabeled_loader, test_loader,
             s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)
 
             # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)
-            dot_product = s_loss_l_old - s_loss_l_new
+            # dot_product = s_loss_l_old - s_loss_l_new
 
             # author's code formula
-            # dot_product = s_loss_l_new - s_loss_l_old
+            dot_product = s_loss_l_new - s_loss_l_old
             # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01
             # dot_product = dot_product - moving_dot_product
 
-            _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)
-            t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)
+            # _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)
+            # t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)
+            #test
+            t_loss_mpl = torch.tensor(0.).to(args.device)
             t_loss = t_loss_uda + t_loss_mpl
 
         t_scaler.scale(t_loss).backward()
@@ -363,6 +365,7 @@ def evaluate(args, test_loader, model, criterion):
 
 
 def finetune(args, train_loader, test_loader, model, criterion):
+    model.drop = nn.Identity()
     train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler
     labeled_loader = DataLoader(
         train_loader.dataset,
@@ -373,7 +376,8 @@ def finetune(args, train_loader, test_loader, model, criterion):
     optimizer = optim.SGD(model.parameters(),
                           lr=args.finetune_lr,
                           momentum=args.finetune_momentum,
-                          weight_decay=args.finetune_weight_decay)
+                          weight_decay=args.finetune_weight_decay,
+                          nesterov=True)
     scaler = amp.GradScaler(enabled=args.amp)
 
     logger.info("***** Running Finetuning *****")
diff --git a/utils.py b/utils.py
index 20b67ea..af3bb40 100644
--- a/utils.py
+++ b/utils.py
@@ -19,10 +19,10 @@ def reduce_tensor(tensor, n):
 
 
 def create_loss_fn(args):
-    if args.label_smoothing > 0:
-        criterion = SmoothCrossEntropy(alpha=args.label_smoothing)
-    else:
-        criterion = nn.CrossEntropyLoss()
+    # if args.label_smoothing > 0:
+    #     criterion = SmoothCrossEntropyV2(alpha=args.label_smoothing)
+    # else:  
+    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
     return criterion.to(args.device)
 
 
@@ -83,12 +83,42 @@ class SmoothCrossEntropy(nn.Module):
         self.alpha = alpha
 
     def forward(self, logits, labels):
-        num_classes = logits.shape[-1]
-        alpha_div_k = self.alpha / num_classes
-        target_probs = F.one_hot(labels, num_classes=num_classes).float() * \
-            (1. - self.alpha) + alpha_div_k
-        loss = -(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)
-        return loss.mean()
+        if self.alpha == 0:
+            loss = F.cross_entropy(logits, labels)
+        else:
+            num_classes = logits.shape[-1]
+            alpha_div_k = self.alpha / num_classes
+            target_probs = F.one_hot(labels, num_classes=num_classes).float() * \
+                (1. - self.alpha) + alpha_div_k
+            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()
+        return loss
+
+
+class SmoothCrossEntropyV2(nn.Module):
+    """
+    NLL loss with label smoothing.
+    """
+
+    def __init__(self, label_smoothing=0.1):
+        """
+        Constructor for the LabelSmoothing module.
+        :param smoothing: label smoothing factor
+        """
+        super().__init__()
+        assert label_smoothing < 1.0
+        self.smoothing = label_smoothing
+        self.confidence = 1. - label_smoothing
+
+    def forward(self, x, target):
+        if self.smoothing == 0:
+            loss = F.cross_entropy(x, target)
+        else:
+            logprobs = F.log_softmax(x, dim=-1)
+            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
+            nll_loss = nll_loss.squeeze(1)
+            smooth_loss = -logprobs.mean(dim=-1)
+            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()
+        return loss
 
 
 class AverageMeter(object):

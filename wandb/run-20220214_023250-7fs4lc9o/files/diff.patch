diff --git a/augmentation.py b/augmentation.py
index 61115ae..c8b8aca 100644
--- a/augmentation.py
+++ b/augmentation.py
@@ -152,14 +152,14 @@ def TranslateY(img, v, max_v, **kwarg):
 
 def TranslateXConst(img, v, max_v, **kwarg):
     v = _float_parameter(v, max_v)
-    if random.random() > 0.5:
+    if random.random() < 0.5:
         v = -v
     return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0), RESAMPLE_MODE, fillcolor=FILL_COLOR)
 
 
 def TranslateYConst(img, v, max_v, **kwarg):
     v = _float_parameter(v, max_v)
-    if random.random() > 0.5:
+    if random.random() < 0.5:
         v = -v
     return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v), RESAMPLE_MODE, fillcolor=FILL_COLOR)
 
diff --git a/data.py b/data.py
index 5eec792..70e518a 100644
--- a/data.py
+++ b/data.py
@@ -5,8 +5,10 @@ import numpy as np
 from PIL import Image
 from torchvision import datasets
 from torchvision import transforms
+from torchvision.transforms import InterpolationMode
 
 from augmentation import RandAugmentCIFAR
+from augment import TrivialAugmentWide
 
 logger = logging.getLogger(__name__)
 
@@ -19,6 +21,10 @@ normal_std = (0.5, 0.5, 0.5)
 
 
 def get_cifar10(args):
+    if args.randaug:
+        n, m = args.randaug
+    else:
+        n, m = 2, 10  # default
     transform_labeled = transforms.Compose([
         transforms.RandomHorizontalFlip(),
         transforms.RandomCrop(size=args.resize,
@@ -26,7 +32,17 @@ def get_cifar10(args):
                               fill=128,
                               padding_mode='constant'),
         transforms.ToTensor(),
-        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)
+        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),
+    ])
+    transform_finetune = transforms.Compose([
+        transforms.RandomHorizontalFlip(),
+        transforms.RandomCrop(size=args.resize,
+                              padding=int(args.resize * 0.125),
+                              fill=128,
+                              padding_mode='constant'),
+        RandAugmentCIFAR(n=n, m=m),
+        transforms.ToTensor(),
+        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),
     ])
     transform_val = transforms.Compose([
         transforms.ToTensor(),
@@ -40,7 +56,10 @@ def get_cifar10(args):
         args.data_path, train_labeled_idxs, train=True,
         transform=transform_labeled
     )
-
+    finetune_dataset = CIFAR10SSL(
+        args.data_path, train_labeled_idxs, train=True,
+        transform=transform_finetune
+    )
     train_unlabeled_dataset = CIFAR10SSL(
         args.data_path, train_unlabeled_idxs,
         train=True,
@@ -50,11 +69,14 @@ def get_cifar10(args):
     test_dataset = datasets.CIFAR10(args.data_path, train=False,
                                     transform=transform_val, download=False)
 
-    return train_labeled_dataset, train_unlabeled_dataset, test_dataset
+    return train_labeled_dataset, train_unlabeled_dataset, test_dataset, finetune_dataset
 
 
 def get_cifar100(args):
-
+    if args.randaug:
+        n, m = args.randaug
+    else:
+        n, m = 2, 10  # default
     transform_labeled = transforms.Compose([
         transforms.RandomHorizontalFlip(),
         transforms.RandomCrop(size=args.resize,
@@ -63,6 +85,15 @@ def get_cifar100(args):
                               padding_mode='constant'),
         transforms.ToTensor(),
         transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])
+    transform_finetune = transforms.Compose([
+        transforms.RandomHorizontalFlip(),
+        transforms.RandomCrop(size=args.resize,
+                              padding=int(args.resize * 0.125),
+                              fill=128,
+                              padding_mode='constant'),
+        RandAugmentCIFAR(n=n, m=m),
+        transforms.ToTensor(),
+        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])
 
     transform_val = transforms.Compose([
         transforms.ToTensor(),
@@ -76,7 +107,10 @@ def get_cifar100(args):
         args.data_path, train_labeled_idxs, train=True,
         transform=transform_labeled
     )
-
+    finetune_dataset = CIFAR100SSL(
+        args.data_path, train_labeled_idxs, train=True,
+        transform=transform_fintune
+    )
     train_unlabeled_dataset = CIFAR100SSL(
         args.data_path, train_unlabeled_idxs, train=True,
         transform=TransformMPL(args, mean=cifar100_mean, std=cifar100_std)
@@ -85,7 +119,7 @@ def get_cifar100(args):
     test_dataset = datasets.CIFAR100(args.data_path, train=False,
                                      transform=transform_val, download=False)
 
-    return train_labeled_dataset, train_unlabeled_dataset, test_dataset
+    return train_labeled_dataset, train_unlabeled_dataset, test_dataset, finetune_dataset
 
 
 def x_u_split(args, labels):
diff --git a/main.py b/main.py
index d3708ac..741333b 100644
--- a/main.py
+++ b/main.py
@@ -56,9 +56,9 @@ parser.add_argument('--resume', default='', type=str, help='path to checkpoint')
 parser.add_argument('--evaluate', action='store_true', help='only evaluate model on validation set')
 parser.add_argument('--finetune', action='store_true',
                     help='only finetune model on labeled dataset')
-parser.add_argument('--finetune-epochs', default=125, type=int, help='finetune epochs')
+parser.add_argument('--finetune-epochs', default=625, type=int, help='finetune epochs')
 parser.add_argument('--finetune-batch-size', default=512, type=int, help='finetune batch size')
-parser.add_argument('--finetune-lr', default=1e-5, type=float, help='finetune learning late')
+parser.add_argument('--finetune-lr', default=3e-5, type=float, help='finetune learning late')
 parser.add_argument('--finetune-weight-decay', default=0, type=float, help='finetune weight decay')
 parser.add_argument('--finetune-momentum', default=0, type=float, help='finetune SGD Momentum')
 parser.add_argument('--seed', default=None, type=int, help='seed for initializing training')
@@ -212,15 +212,17 @@ def train_loop(args, labeled_loader, unlabeled_loader, test_loader,
             s_loss_l_new = F.cross_entropy(s_logits_l.detach(), targets)
 
             # theoretically correct formula (https://github.com/kekmodel/MPL-pytorch/issues/6)
-            dot_product = s_loss_l_old - s_loss_l_new
+            # dot_product = s_loss_l_old - s_loss_l_new
 
             # author's code formula
-            # dot_product = s_loss_l_new - s_loss_l_old
+            dot_product = s_loss_l_new - s_loss_l_old
             # moving_dot_product = moving_dot_product * 0.99 + dot_product * 0.01
             # dot_product = dot_product - moving_dot_product
 
             _, hard_pseudo_label = torch.max(t_logits_us.detach(), dim=-1)
             t_loss_mpl = dot_product * F.cross_entropy(t_logits_us, hard_pseudo_label)
+            # test
+            # t_loss_mpl = torch.tensor(0.).to(args.device)
             t_loss = t_loss_uda + t_loss_mpl
 
         t_scaler.scale(t_loss).backward()
@@ -362,10 +364,11 @@ def evaluate(args, test_loader, model, criterion):
         return losses.avg, top1.avg, top5.avg
 
 
-def finetune(args, train_loader, test_loader, model, criterion):
+def finetune(args, finetune_dataset, test_loader, model, criterion):
+    model.drop = nn.Identity()
     train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler
     labeled_loader = DataLoader(
-        train_loader.dataset,
+        finetune_dataset,
         sampler=train_sampler(train_loader.dataset),
         batch_size=args.finetune_batch_size,
         num_workers=args.workers,
@@ -373,7 +376,8 @@ def finetune(args, train_loader, test_loader, model, criterion):
     optimizer = optim.SGD(model.parameters(),
                           lr=args.finetune_lr,
                           momentum=args.finetune_momentum,
-                          weight_decay=args.finetune_weight_decay)
+                          weight_decay=args.finetune_weight_decay,
+                          nesterov=True)
     scaler = amp.GradScaler(enabled=args.amp)
 
     logger.info("***** Running Finetuning *****")
@@ -482,7 +486,7 @@ def main():
     if args.local_rank not in [-1, 0]:
         torch.distributed.barrier()
 
-    labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS[args.dataset](args)
+    labeled_dataset, unlabeled_dataset, test_dataset, finetune_dataset = DATASET_GETTERS[args.dataset](args)
 
     if args.local_rank == 0:
         torch.distributed.barrier()
@@ -494,7 +498,7 @@ def main():
         batch_size=args.batch_size,
         num_workers=args.workers,
         drop_last=True)
-
+    
     unlabeled_loader = DataLoader(
         unlabeled_dataset,
         sampler=train_sampler(unlabeled_dataset),
@@ -615,7 +619,7 @@ def main():
     if args.finetune:
         del t_scaler, t_scheduler, t_optimizer, teacher_model, unlabeled_loader
         del s_scaler, s_scheduler, s_optimizer
-        finetune(args, labeled_loader, test_loader, student_model, criterion)
+        finetune(args, fintune_dataset, test_loader, student_model, criterion)
         return
 
     if args.evaluate:
diff --git a/utils.py b/utils.py
index 20b67ea..af3bb40 100644
--- a/utils.py
+++ b/utils.py
@@ -19,10 +19,10 @@ def reduce_tensor(tensor, n):
 
 
 def create_loss_fn(args):
-    if args.label_smoothing > 0:
-        criterion = SmoothCrossEntropy(alpha=args.label_smoothing)
-    else:
-        criterion = nn.CrossEntropyLoss()
+    # if args.label_smoothing > 0:
+    #     criterion = SmoothCrossEntropyV2(alpha=args.label_smoothing)
+    # else:  
+    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
     return criterion.to(args.device)
 
 
@@ -83,12 +83,42 @@ class SmoothCrossEntropy(nn.Module):
         self.alpha = alpha
 
     def forward(self, logits, labels):
-        num_classes = logits.shape[-1]
-        alpha_div_k = self.alpha / num_classes
-        target_probs = F.one_hot(labels, num_classes=num_classes).float() * \
-            (1. - self.alpha) + alpha_div_k
-        loss = -(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)
-        return loss.mean()
+        if self.alpha == 0:
+            loss = F.cross_entropy(logits, labels)
+        else:
+            num_classes = logits.shape[-1]
+            alpha_div_k = self.alpha / num_classes
+            target_probs = F.one_hot(labels, num_classes=num_classes).float() * \
+                (1. - self.alpha) + alpha_div_k
+            loss = (-(target_probs * torch.log_softmax(logits, dim=-1)).sum(dim=-1)).mean()
+        return loss
+
+
+class SmoothCrossEntropyV2(nn.Module):
+    """
+    NLL loss with label smoothing.
+    """
+
+    def __init__(self, label_smoothing=0.1):
+        """
+        Constructor for the LabelSmoothing module.
+        :param smoothing: label smoothing factor
+        """
+        super().__init__()
+        assert label_smoothing < 1.0
+        self.smoothing = label_smoothing
+        self.confidence = 1. - label_smoothing
+
+    def forward(self, x, target):
+        if self.smoothing == 0:
+            loss = F.cross_entropy(x, target)
+        else:
+            logprobs = F.log_softmax(x, dim=-1)
+            nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
+            nll_loss = nll_loss.squeeze(1)
+            smooth_loss = -logprobs.mean(dim=-1)
+            loss = (self.confidence * nll_loss + self.smoothing * smooth_loss).mean()
+        return loss
 
 
 class AverageMeter(object):
